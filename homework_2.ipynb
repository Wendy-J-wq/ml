{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<h1>\n",
    "<center>CFRM 421/521, Spring 2022</center>\n",
    "</h1>\n",
    "\n",
    "<h1>\n",
    "<center>Wendy Jiang</center>\n",
    "</h1>\n",
    "\n",
    "<h1>\n",
    "<center>Homework 2</center>\n",
    "</h1>\n",
    "\n",
    "* **Due: Monday, May 2, 2022, 11:59 PM**\n",
    "\n",
    "\n",
    "* Total marks: 41\n",
    "\n",
    "\n",
    "* Late submissions are allowed, but a 20% penalty per day applies. Your last submission is considered for calculating the penalty.\n",
    "\n",
    "\n",
    "*  Use this Jupyter notebook as a template for your solutions. **Your solution must be submitted as one Jupyter notebook on Canvas and one PDF file on Gradescope.** The notebook must be already run, that is, make sure that you have run all your code, save the notebook, and then when you reopen the notebook, checked that all output appears as expected. You are allowed to use code from the textbook, textbook website, or lecture notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Random forest for time series data [13 marks]\n",
    "\n",
    "In this question you will work with the NYSE dataset. Only 3 time series in this dataset will be use: `DJ_return` ($a_t$), `log_volatility` ($b_t$), and `log_volume` ($c_t$). Download the data as a csv file from [Canvas](https://canvas.uw.edu/files/91091313/download?download_frd=1). The data was originally obtained from the R library ISLR2, and you can read the documentation for the dataset [here](https://cran.rstudio.com/web/packages/ISLR2/ISLR2.pdf), which explains the meaning of the variables.\n",
    "\n",
    "You want to predict the 1-step ahead value of `log_volume` $c_{t+1}$ using the previous values of this variable and the other two variables (`DJ_return` and `log_volatility`) up to 5 lags. So the features are $c_{t},\\dots,c_{t-4},b_{t},\\dots,b_{t-4},a_{t},\\dots,a_{t-4}$.\n",
    "\n",
    "If the data is stored in a file named `NYSE.csv` in your working directory, then loading the data can be done using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"NYSE.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [3 marks]\n",
    "\n",
    "Create the feature matrix `X` and the target variable `y`. Print at least the first 2 rows of `X` and `y` (it is acceptable that not every element of the rows are printed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>DJ_return</th>\n",
       "      <th>log_volume</th>\n",
       "      <th>log_volatility</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1962-12-03</td>\n",
       "      <td>mon</td>\n",
       "      <td>-0.004461</td>\n",
       "      <td>0.032573</td>\n",
       "      <td>-13.127403</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1962-12-04</td>\n",
       "      <td>tues</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>0.346202</td>\n",
       "      <td>-11.749305</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1962-12-05</td>\n",
       "      <td>wed</td>\n",
       "      <td>0.003845</td>\n",
       "      <td>0.525306</td>\n",
       "      <td>-11.665609</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1962-12-06</td>\n",
       "      <td>thur</td>\n",
       "      <td>-0.003462</td>\n",
       "      <td>0.210182</td>\n",
       "      <td>-11.626772</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1962-12-07</td>\n",
       "      <td>fri</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.044187</td>\n",
       "      <td>-11.728130</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date day_of_week  DJ_return  log_volume  log_volatility  train\n",
       "0  1962-12-03         mon  -0.004461    0.032573      -13.127403   True\n",
       "1  1962-12-04        tues   0.007813    0.346202      -11.749305   True\n",
       "2  1962-12-05         wed   0.003845    0.525306      -11.665609   True\n",
       "3  1962-12-06        thur  -0.003462    0.210182      -11.626772   True\n",
       "4  1962-12-07         fri   0.000568    0.044187      -11.728130   True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data[['DJ_return','log_volume','log_volatility']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def ts_split(ts, feature_steps=5, target_steps=1):\n",
    "    n_obs = len(ts) - feature_steps - target_steps + 1\n",
    "    X = np.array([ts[idx:idx + feature_steps] for idx in range(n_obs)])\n",
    "    y = np.array([ts[idx + feature_steps:idx + feature_steps + target_steps]\n",
    "                  for idx in range(n_obs)])\n",
    "    return X, y\n",
    "\n",
    "X, y = ts_split(data1, feature_steps=5, target_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-4.46100000e-03  3.25730000e-02 -1.31274026e+01]\n",
      "  [ 7.81300000e-03  3.46202000e-01 -1.17493047e+01]\n",
      "  [ 3.84500000e-03  5.25306000e-01 -1.16656090e+01]\n",
      "  [-3.46200000e-03  2.10182000e-01 -1.16267724e+01]\n",
      "  [ 5.68000000e-04  4.41870000e-02 -1.17281302e+01]]\n",
      "\n",
      " [[ 7.81300000e-03  3.46202000e-01 -1.17493047e+01]\n",
      "  [ 3.84500000e-03  5.25306000e-01 -1.16656090e+01]\n",
      "  [-3.46200000e-03  2.10182000e-01 -1.16267724e+01]\n",
      "  [ 5.68000000e-04  4.41870000e-02 -1.17281302e+01]\n",
      "  [-1.08240000e-02  1.33246000e-01 -1.08725263e+01]]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:2,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (y[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.133246]\n",
      " [-0.011528]]\n"
     ]
    }
   ],
   "source": [
    "print(y[:2,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6046, 1)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6046, 5, 3)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)  [4 marks]\n",
    "\n",
    "Consider fitting a random forest to predict the 1-step ahead value of `log_volume`. The random forest must include the argument `random_state=42`, and it is useful to also include `n_jobs=-1` (you can use `n_job=-1` throughout this homework wherever it is avaliable). Use 3-fold time series CV, with the test set split 50% into a validation set and 50% into the actual test set, to tune the hyperparameters `n_estimators` taking the values  100, 500, 750, and the cost-complexity pruning parameter $\\alpha$ taking the values $10^{-k}$, $k=0,1,\\dots,9$. The performance measure is RMSE. Report the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1aheadonly = X\n",
    "y_1aheadonly = y\n",
    "n_samples,nx,ny = X_1aheadonly.shape\n",
    "X_1aheadonly = X_1aheadonly.reshape((n_samples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6046, 1)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1aheadonly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.133246],\n",
       "       [-0.011528],\n",
       "       [ 0.001607],\n",
       "       ...,\n",
       "       [-0.371237],\n",
       "       [-0.385638],\n",
       "       [-0.264986]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1aheadonly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.46100000e-03,  3.25730000e-02, -1.31274026e+01, ...,\n",
       "         5.68000000e-04,  4.41870000e-02, -1.17281302e+01],\n",
       "       [ 7.81300000e-03,  3.46202000e-01, -1.17493047e+01, ...,\n",
       "        -1.08240000e-02,  1.33246000e-01, -1.08725263e+01],\n",
       "       [ 3.84500000e-03,  5.25306000e-01, -1.16656090e+01, ...,\n",
       "         1.24000000e-04, -1.15280000e-02, -1.09777968e+01],\n",
       "       ...,\n",
       "       [ 8.34500000e-03,  9.86704000e-01, -9.64413783e+00, ...,\n",
       "         1.82500000e-03, -1.32242500e+00, -9.90602537e+00],\n",
       "       [-1.38500000e-03,  3.66152000e-01, -9.74621375e+00, ...,\n",
       "        -9.51500000e-03, -3.71237000e-01, -9.82766012e+00],\n",
       "       [-6.15000000e-03,  4.50780000e-01, -9.78221391e+00, ...,\n",
       "        -1.83700000e-03, -3.85638000e-01, -9.92609069e+00]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1aheadonly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "series_len = data1.size\n",
    "# Hyperparamter combinations\n",
    "n_estimators_list = [100,500,750]\n",
    "\n",
    "ccp_alpha_list = [pow(10,-1*0),pow(10,-1*1),pow(10,-1*2),pow(10,-1*3),\n",
    "                 pow(10,-1*4),pow(10,-1*5),pow(10,-1*6),pow(10,-1*7),\n",
    "                 pow(10,-1*8),pow(10,-1*9)]\n",
    "\n",
    "def time_series_valid_test(X, y, n_split, valid_or_test, optimal_par=None):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_split)\n",
    "    rf_rmse = []\n",
    "    i = 0\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        i += 1\n",
    "        # Break test set into 50% validation set, 50% test set\n",
    "        break_test_ind = int(test_index[0] + 0.5*(test_index[-1]-test_index[0]))\n",
    "        valid_index = np.array(list(range(test_index[0],break_test_ind)))\n",
    "        test_index = np.array(list(range(break_test_ind,test_index[-1])))\n",
    "\n",
    "        # Split\n",
    "        X_train, X_valid, X_test = X[train_index], X[valid_index], X[test_index]\n",
    "        y_train, y_valid, y_test = y[train_index], y[valid_index], y[test_index]\n",
    "\n",
    "        # Tuning\n",
    "        if valid_or_test == \"valid\":\n",
    "            for ccp_alpha in ccp_alpha_list:\n",
    "                for n_estimators in n_estimators_list:\n",
    "                    model_rf = RandomForestRegressor(random_state=42, \n",
    "                               ccp_alpha = ccp_alpha, n_estimators=n_estimators)\n",
    "                    model_rf.fit(X_train, y_train.ravel())\n",
    "                    y_val_rf = model_rf.predict(X_valid)\n",
    "                    rf_rmse.append(np.sqrt(mean_squared_error(y_valid, y_val_rf)))\n",
    "        \n",
    "        # Evalulate on test set\n",
    "        if valid_or_test == \"test\":\n",
    "            model_rf = RandomForestRegressor(random_state=42, \n",
    "                       ccp_alpha=optimal_par[0], n_estimators=optimal_par[1])\n",
    "            model_rf.fit(X_train, y_train.ravel())\n",
    "            y_test_rf = model_rf.predict(X_test)\n",
    "            rf_rmse.append(np.sqrt(mean_squared_error(y_test, y_test_rf)))\n",
    "            \n",
    "            # Plot the prediction for the last CV fold\n",
    "            if i == n_split:\n",
    "                plt.plot(range(series_len-test_index.size,series_len),\n",
    "                         y_test_rf, label=\"1-steps ahead prediction\")\n",
    "                plt.plot(range(series_len-test_index.size,series_len),\n",
    "                         y_test, label=\"True value\")\n",
    "                plt.legend(loc=\"upper left\")\n",
    "    \n",
    "    # Average RMSE over CV folds\n",
    "    if valid_or_test == \"valid\":\n",
    "        rf_rmse = np.mean(np.array(rf_rmse).reshape(\n",
    "            n_split, len(ccp_alpha_list)*len(n_estimators_list)), axis=0)\n",
    "        return rf_rmse\n",
    "    if valid_or_test == \"test\":\n",
    "        rf_rmse = np.mean(rf_rmse)\n",
    "        return rf_rmse, y_test_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(ccp_alpha, n_estimators):', [1, 100]]\n",
      "0.2358102502325389\n",
      "['(ccp_alpha, n_estimators):', [1, 500]]\n",
      "0.23583279057420017\n",
      "['(ccp_alpha, n_estimators):', [1, 750]]\n",
      "0.23582412722547763\n",
      "['(ccp_alpha, n_estimators):', [0.1, 100]]\n",
      "0.2358102502325389\n",
      "['(ccp_alpha, n_estimators):', [0.1, 500]]\n",
      "0.23583279057420017\n",
      "['(ccp_alpha, n_estimators):', [0.1, 750]]\n",
      "0.23582412722547763\n",
      "['(ccp_alpha, n_estimators):', [0.01, 100]]\n",
      "0.1931776501341703\n",
      "['(ccp_alpha, n_estimators):', [0.01, 500]]\n",
      "0.19289278821198516\n",
      "['(ccp_alpha, n_estimators):', [0.01, 750]]\n",
      "0.1928692930007393\n",
      "['(ccp_alpha, n_estimators):', [0.001, 100]]\n",
      "0.1742571823815323\n",
      "['(ccp_alpha, n_estimators):', [0.001, 500]]\n",
      "0.1741139415851224\n",
      "['(ccp_alpha, n_estimators):', [0.001, 750]]\n",
      "0.1740238054718721\n",
      "['(ccp_alpha, n_estimators):', [0.0001, 100]]\n",
      "0.16135293770866693\n",
      "['(ccp_alpha, n_estimators):', [0.0001, 500]]\n",
      "0.1610553983691199\n",
      "['(ccp_alpha, n_estimators):', [0.0001, 750]]\n",
      "0.16115663900992447\n",
      "['(ccp_alpha, n_estimators):', [1e-05, 100]]\n",
      "0.16111597745843975\n",
      "['(ccp_alpha, n_estimators):', [1e-05, 500]]\n",
      "0.16047851695529133\n",
      "['(ccp_alpha, n_estimators):', [1e-05, 750]]\n",
      "0.16049997955185025\n",
      "['(ccp_alpha, n_estimators):', [1e-06, 100]]\n",
      "0.16147248708876136\n",
      "['(ccp_alpha, n_estimators):', [1e-06, 500]]\n",
      "0.1605719451884491\n",
      "['(ccp_alpha, n_estimators):', [1e-06, 750]]\n",
      "0.16058617165836728\n",
      "['(ccp_alpha, n_estimators):', [1e-07, 100]]\n",
      "0.1615109410650685\n",
      "['(ccp_alpha, n_estimators):', [1e-07, 500]]\n",
      "0.16056503205113146\n",
      "['(ccp_alpha, n_estimators):', [1e-07, 750]]\n",
      "0.1605751748270485\n",
      "['(ccp_alpha, n_estimators):', [1e-08, 100]]\n",
      "0.16152158547493112\n",
      "['(ccp_alpha, n_estimators):', [1e-08, 500]]\n",
      "0.16056854272210067\n",
      "['(ccp_alpha, n_estimators):', [1e-08, 750]]\n",
      "0.16057799765721328\n",
      "['(ccp_alpha, n_estimators):', [1e-09, 100]]\n",
      "0.1615196461934217\n",
      "['(ccp_alpha, n_estimators):', [1e-09, 500]]\n",
      "0.16056802313453636\n",
      "['(ccp_alpha, n_estimators):', [1e-09, 750]]\n",
      "0.1605779768989329\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "rf_rmse = time_series_valid_test(X_1aheadonly, y_1aheadonly, 3, \"valid\")\n",
    "n_jobs=-1\n",
    "ind = 0\n",
    "for ccp_alpha in ccp_alpha_list:\n",
    "        for n_estimators in n_estimators_list:\n",
    "            print([\"(ccp_alpha, n_estimators):\",[ccp_alpha,n_estimators]])\n",
    "            print(rf_rmse[ind])\n",
    "            ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best hyperparameters:['(ccp_alpha, n_estimators):', [1e-05, 500]]\n",
    "0.16047851695529133"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)  [2 marks]\n",
    "\n",
    "Using the same time series split as in (b), compute the RMSE of the best fitting model on the test set, and include a plot of the true values and predicted values on the test set of the last fold (the fold closest to the current time) of the CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18700029150790354"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "rf_rmse, y_test_rf = time_series_valid_test(X_1aheadonly, y_1aheadonly, 3, \"test\", [1e-05, 500])\n",
    "rf_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model's parameter is ccp_alpha = 1e-5 and n_estimators = 500, fitting it on the test set, the RMSE is around 0.187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) [2 marks]\n",
    "\n",
    "It is often useful to check that your model is not worse than a very simple method of prediction. Compute the RMSE of a model that simply predicts the 1-step ahead value of `log_volume` $c_{t+1}$ as the current value $c_t$, and compare this to the best fitting random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_split(ts, feature_steps=1, target_steps=1):\n",
    "    n_obs = len(ts) - feature_steps - target_steps + 1\n",
    "    X_log_v = np.array([ts[idx:idx + feature_steps] for idx in range(n_obs)])\n",
    "    y_log_v = np.array([ts[idx + feature_steps:idx + feature_steps + target_steps]\n",
    "                  for idx in range(n_obs)])\n",
    "    return X_log_v[:,:,[3]], y_log_v[:,:,[3]]\n",
    "\n",
    "X_log_v, y_log_v = ts_split(data, feature_steps=1, target_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2aheadonly = X_log_v\n",
    "y_2aheadonly = y_log_v\n",
    "n_samples1,nx1,ny1 = X_2aheadonly.shape\n",
    "X_2aheadonly = X_2aheadonly.reshape((n_samples1,nx1*ny1))\n",
    "n_samples2,nx2,ny2 = y_2aheadonly.shape\n",
    "y_2aheadonly = y_2aheadonly.reshape((n_samples2,nx2*ny2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ind = int(X.shape[0]*0.8)\n",
    "X_2aheadonly_train_full, y_2aheadonly_train_full = X_2aheadonly[:split_ind_2], y_2aheadonly[:split_ind_2]\n",
    "X_test, y_test = X_2aheadonly[split_ind:], y_2aheadonly[split_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_2aheadonly_train_full, y_2aheadonly_train_full.ravel())\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2098217358241793"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE of a model that simply predicts the 1-step ahead value of log_volume is 0.2098, which is little bit bigger than the best fitting random forest model's RMSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) [2 marks]\n",
    "\n",
    "Compute the feature importances of the best fitting model. Which feature is the most important and what is its feature importance value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0338198 , 0.05925312, 0.01567865, 0.02981184, 0.06281708,\n",
       "       0.01206384, 0.03695308, 0.04355261, 0.01099052, 0.03901734,\n",
       "       0.03701707, 0.01281687, 0.06401678, 0.52835805, 0.01383337])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=500,\n",
    "                                ccp_alpha = 1e-5,\n",
    "                                random_state = 42,\n",
    "                                n_jobs=-1)\n",
    "forest.fit(X_1aheadonly, y_1aheadonly.ravel())\n",
    "\n",
    "\n",
    "forest.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are $a_{t},\\dots,a_{t-4},c_{t},\\dots,c_{t-4},b_{t},\\dots,b_{t-4}$, thus the most important feature is $b$, and its importance value is 0.52835805."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SVM classification and regression [11 marks]\n",
    "\n",
    "## (a) [2 marks]\n",
    "\n",
    "In this question, a SVM is used for classification for the MNIST dataset. The following code loads the MNIST dataset, creates the test set, and to reduce training time, takes a random sample of 2000 points from the full training set to use as your actual training set stored in `X` and `y`. Do not shuffle the data and do not use a standard scaler.\n",
    "\n",
    "Hint: Reading the solution to Question 9 in the Chapter 5 [Jupyter notebook](https://github.com/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb) on the textbook website may help with this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, cache=True)\n",
    "mnist.target = mnist.target.astype(np.int8)\n",
    "X_train = mnist[\"data\"][:60000]\n",
    "X_test  = mnist[\"data\"][60000:]\n",
    "y_train = mnist[\"target\"][:60000]\n",
    "y_test  = mnist[\"target\"][60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "N = 2000\n",
    "split_obj = StratifiedShuffleSplit(n_splits=1,\n",
    "                               test_size=N/60000, random_state=42)\n",
    "for other_idx, subsample_idx in split_obj.split(X_train, y_train):\n",
    "    X = X_train[subsample_idx]\n",
    "    y = y_train[subsample_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Consider fitting the linear SVM classifier (`LinearSVC`) with `max_iter=50000`. For this model, optimize the hyperparameter $C$ using 3-fold CV over the values $10^{-k}$, $k=0,1,\\dots,9$, where the performance measure is accuracy. What is the best $C$ and what is the accuracy in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=50000, random_state=42)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "X_train_new = X\n",
    "y_train_new = y\n",
    "lin_clf = LinearSVC(max_iter=50000,random_state=42)\n",
    "lin_clf.fit(X_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] C=1 .............................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............................................. C=1, total=   0.7s\n",
      "[CV] C=1 .............................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............................................. C=1, total=   0.7s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] .............................................. C=1, total=   0.6s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................................ C=0.1, total=   0.6s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................................ C=0.1, total=   0.7s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................................ C=0.1, total=   0.6s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................................... C=0.01, total=   0.6s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................................... C=0.01, total=   0.7s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........................................... C=0.01, total=   0.6s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................................... C=0.001, total=   0.6s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................................... C=0.001, total=   0.7s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......................................... C=0.001, total=   0.5s\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................................... C=0.0001, total=   0.5s\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................................... C=0.0001, total=   0.6s\n",
      "[CV] C=0.0001 ........................................................\n",
      "[CV] ......................................... C=0.0001, total=   0.5s\n",
      "[CV] C=1e-05 .........................................................\n",
      "[CV] .......................................... C=1e-05, total=   0.3s\n",
      "[CV] C=1e-05 .........................................................\n",
      "[CV] .......................................... C=1e-05, total=   0.3s\n",
      "[CV] C=1e-05 .........................................................\n",
      "[CV] .......................................... C=1e-05, total=   0.3s\n",
      "[CV] C=1e-06 .........................................................\n",
      "[CV] .......................................... C=1e-06, total=   0.1s\n",
      "[CV] C=1e-06 .........................................................\n",
      "[CV] .......................................... C=1e-06, total=   0.1s\n",
      "[CV] C=1e-06 .........................................................\n",
      "[CV] .......................................... C=1e-06, total=   0.1s\n",
      "[CV] C=1e-07 .........................................................\n",
      "[CV] .......................................... C=1e-07, total=   0.0s\n",
      "[CV] C=1e-07 .........................................................\n",
      "[CV] .......................................... C=1e-07, total=   0.0s\n",
      "[CV] C=1e-07 .........................................................\n",
      "[CV] .......................................... C=1e-07, total=   0.0s\n",
      "[CV] C=1e-08 .........................................................\n",
      "[CV] .......................................... C=1e-08, total=   0.0s\n",
      "[CV] C=1e-08 .........................................................\n",
      "[CV] .......................................... C=1e-08, total=   0.1s\n",
      "[CV] C=1e-08 .........................................................\n",
      "[CV] .......................................... C=1e-08, total=   0.0s\n",
      "[CV] C=1e-09 .........................................................\n",
      "[CV] .......................................... C=1e-09, total=   0.0s\n",
      "[CV] C=1e-09 .........................................................\n",
      "[CV] .......................................... C=1e-09, total=   0.0s\n",
      "[CV] C=1e-09 .........................................................\n",
      "[CV] .......................................... C=1e-09, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   10.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LinearSVC(max_iter=50000, random_state=42),\n",
       "                   param_distributions={'C': [1, 0.1, 0.01, 0.001, 0.0001,\n",
       "                                              1e-05, 1e-06, 1e-07, 1e-08,\n",
       "                                              1e-09]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "param_distributions = {\"C\": [pow(10,-1*0), pow(10,-1*1),pow(10,-1*2),\n",
    "                             pow(10,-1*3), pow(10,-1*4),pow(10,-1*5),\n",
    "                             pow(10,-1*6), pow(10,-1*7),pow(10,-1*8),pow(10,-1*9)]}\n",
    "rnd_search_cv = RandomizedSearchCV(lin_clf, param_distributions, n_iter=10, verbose=2, cv=3, random_state=42)\n",
    "rnd_search_cv.fit(X_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1e-07, max_iter=50000, random_state=42)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:278: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] C=1e-07 .........................................................\n",
      "[CV] .......................................... C=1e-07, total=   0.1s\n",
      "[CV] C=1e-07 .........................................................\n",
      "[CV] .......................................... C=1e-07, total=   0.0s\n",
      "[CV] C=1e-07 .........................................................\n",
      "[CV] .......................................... C=1e-07, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.926"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "param_dist = {\"C\": [pow(10,-1*7)]}\n",
    "rnd_search_cv = RandomizedSearchCV(lin_clf, param_dist, verbose=2, cv=3)\n",
    "rnd_search_cv.fit(X_train_new, y_train_new)\n",
    "y_pred_2 = rnd_search_cv.predict(X_train_new)\n",
    "accuracy_score(y_train_new, y_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best $C = {10}^{-7}$, and in this case the accuracy is 0.926"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [2 marks]\n",
    "\n",
    "**Task:** Now consider fitting a SVM with a Gaussian RBF kernel and `max_iter=50000`. For this model, optimize the hyperparameters $C$ over the distrbution `uniform(1,10)` and $\\gamma$ over the distribution `reciprocal(0.001, 0.1)` with 10 random samples. Again, use 3-fold CV and the performance measure is accuracy. What are the best hyperparameters and what is the accuracy in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] C=2.9060489549325355, gamma=0.03546640867963035 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. C=2.9060489549325355, gamma=0.03546640867963035, total=   3.0s\n",
      "[CV] C=2.9060489549325355, gamma=0.03546640867963035 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. C=2.9060489549325355, gamma=0.03546640867963035, total=   3.0s\n",
      "[CV] C=2.9060489549325355, gamma=0.03546640867963035 .................\n",
      "[CV] .. C=2.9060489549325355, gamma=0.03546640867963035, total=   3.0s\n",
      "[CV] C=6.3956351870026245, gamma=0.001410697586422265 ................\n",
      "[CV] . C=6.3956351870026245, gamma=0.001410697586422265, total=   1.6s\n",
      "[CV] C=6.3956351870026245, gamma=0.001410697586422265 ................\n",
      "[CV] . C=6.3956351870026245, gamma=0.001410697586422265, total=   1.6s\n",
      "[CV] C=6.3956351870026245, gamma=0.001410697586422265 ................\n",
      "[CV] . C=6.3956351870026245, gamma=0.001410697586422265, total=   1.5s\n",
      "[CV] C=3.366641869729813, gamma=0.04939335286997754 ..................\n",
      "[CV] ... C=3.366641869729813, gamma=0.04939335286997754, total=   3.0s\n",
      "[CV] C=3.366641869729813, gamma=0.04939335286997754 ..................\n",
      "[CV] ... C=3.366641869729813, gamma=0.04939335286997754, total=   3.0s\n",
      "[CV] C=3.366641869729813, gamma=0.04939335286997754 ..................\n",
      "[CV] ... C=3.366641869729813, gamma=0.04939335286997754, total=   3.0s\n",
      "[CV] C=7.22230436028801, gamma=0.04399680701575934 ...................\n",
      "[CV] .... C=7.22230436028801, gamma=0.04399680701575934, total=   3.0s\n",
      "[CV] C=7.22230436028801, gamma=0.04399680701575934 ...................\n",
      "[CV] .... C=7.22230436028801, gamma=0.04399680701575934, total=   3.0s\n",
      "[CV] C=7.22230436028801, gamma=0.04399680701575934 ...................\n",
      "[CV] .... C=7.22230436028801, gamma=0.04399680701575934, total=   3.0s\n",
      "[CV] C=5.521371819026276, gamma=0.04582307250562502 ..................\n",
      "[CV] ... C=5.521371819026276, gamma=0.04582307250562502, total=   3.0s\n",
      "[CV] C=5.521371819026276, gamma=0.04582307250562502 ..................\n",
      "[CV] ... C=5.521371819026276, gamma=0.04582307250562502, total=   3.0s\n",
      "[CV] C=5.521371819026276, gamma=0.04582307250562502 ..................\n",
      "[CV] ... C=5.521371819026276, gamma=0.04582307250562502, total=   3.0s\n",
      "[CV] C=5.635573260606385, gamma=0.0011918276449898906 ................\n",
      "[CV] . C=5.635573260606385, gamma=0.0011918276449898906, total=   1.5s\n",
      "[CV] C=5.635573260606385, gamma=0.0011918276449898906 ................\n",
      "[CV] . C=5.635573260606385, gamma=0.0011918276449898906, total=   1.4s\n",
      "[CV] C=5.635573260606385, gamma=0.0011918276449898906 ................\n",
      "[CV] . C=5.635573260606385, gamma=0.0011918276449898906, total=   1.4s\n",
      "[CV] C=10.862384448518496, gamma=0.0010001877430683326 ...............\n",
      "[CV]  C=10.862384448518496, gamma=0.0010001877430683326, total=   1.4s\n",
      "[CV] C=10.862384448518496, gamma=0.0010001877430683326 ...............\n",
      "[CV]  C=10.862384448518496, gamma=0.0010001877430683326, total=   1.3s\n",
      "[CV] C=10.862384448518496, gamma=0.0010001877430683326 ...............\n",
      "[CV]  C=10.862384448518496, gamma=0.0010001877430683326, total=   1.3s\n",
      "[CV] C=8.180764662117685, gamma=0.0011235200516256773 ................\n",
      "[CV] . C=8.180764662117685, gamma=0.0011235200516256773, total=   1.5s\n",
      "[CV] C=8.180764662117685, gamma=0.0011235200516256773 ................\n",
      "[CV] . C=8.180764662117685, gamma=0.0011235200516256773, total=   1.4s\n",
      "[CV] C=8.180764662117685, gamma=0.0011235200516256773 ................\n",
      "[CV] . C=8.180764662117685, gamma=0.0011235200516256773, total=   1.4s\n",
      "[CV] C=3.064934942990443, gamma=0.034072164893250656 .................\n",
      "[CV] .. C=3.064934942990443, gamma=0.034072164893250656, total=   3.0s\n",
      "[CV] C=3.064934942990443, gamma=0.034072164893250656 .................\n",
      "[CV] .. C=3.064934942990443, gamma=0.034072164893250656, total=   3.0s\n",
      "[CV] C=3.064934942990443, gamma=0.034072164893250656 .................\n",
      "[CV] .. C=3.064934942990443, gamma=0.034072164893250656, total=   3.0s\n",
      "[CV] C=5.4196561481526135, gamma=0.001366997785406688 ................\n",
      "[CV] . C=5.4196561481526135, gamma=0.001366997785406688, total=   1.6s\n",
      "[CV] C=5.4196561481526135, gamma=0.001366997785406688 ................\n",
      "[CV] . C=5.4196561481526135, gamma=0.001366997785406688, total=   1.6s\n",
      "[CV] C=5.4196561481526135, gamma=0.001366997785406688 ................\n",
      "[CV] . C=5.4196561481526135, gamma=0.001366997785406688, total=   1.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=SVC(max_iter=50000),\n",
       "                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fc5582f69d0>,\n",
       "                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fc5582eac70>},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "rbf_kernel_clf = SVC(kernel=\"rbf\", max_iter=50000)\n",
    "rbf_kernel_clf.fit(X_train_new, y_train_new)\n",
    "param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\n",
    "rnd_search_cv = RandomizedSearchCV(rbf_kernel_clf, param_distributions, n_iter=10, \n",
    "                                   #scoring = 'accuracy,'\n",
    "                                   verbose=2, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=5.635573260606385, gamma=0.0011918276449898906, max_iter=50000)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_estimator_.fit(X_train, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_3 = rnd_search_cv.predict(X_train)\n",
    "accuracy_score(y_train_new, y_pred_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the best hyperparameters are C=5.635573260606385, gamma=0.0011918276449898906, and the accuracy is 0.9995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [2 mark]\n",
    "\n",
    "**Task:** Choose the best model in (a) and (b). Then for this model, evaluate the accuracy on the test set, which is stored in `X_test` and `y_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is the SVM with a Gaussian RBF kernel model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9124"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "X_test_stand = scalar.transform(X_test)\n",
    "y_pred = rnd_search_cv.predict(X_test_stand)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the accuracy of SVM with RBF kernal is 0.9124."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) [3 marks]\n",
    "\n",
    "Consider the California housing data from Homework 1 using the same training and test set there. The data is obtained using the code below, which comes from Homework 1, and the training set is stored in `X` and `y`. Do not shuffle the data.\n",
    "\n",
    "Hint: Reading the solution to Question 10 in the Chapter 5 [Jupyter notebook](https://github.com/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb) on the textbook website may help with this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "\n",
    "def fetch_housing_data(housing_url, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "HOUSING_URL = (\"https://raw.githubusercontent.com/ageron/\"+\n",
    "               \"handson-ml2/master/datasets/housing/housing.tgz\")\n",
    "fetch_housing_data(HOUSING_URL)\n",
    "data = load_housing_data()\n",
    "\n",
    "data[\"income_cat\"] = np.ceil(data[\"median_income\"] / 1.5)\n",
    "data[\"income_cat\"].where(data[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(data, data[\"income_cat\"]):\n",
    "    strat_train_set = data.loc[train_index]\n",
    "    strat_test_set = data.loc[test_index]\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "X_raw = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "y = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "num_features = X_raw.drop(\"ocean_proximity\", axis=1)\n",
    "num_attribs = list(num_features)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "X = full_pipeline.fit_transform(X_raw)\n",
    "X_test_raw = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "X_test = full_pipeline.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Consider SVM regression with a Gaussian RBF kernel and a sigmoid kernel with `max_iter=50000`. For both models, use randomized search to choose good hyperparameter values for `C` and `gamma`, and set the arguement `random_state=42`. For both models, optimize the hyperparameters $C$ over the distrbution `uniform(1,10)` and $\\gamma$ over the distribution `reciprocal(0.001, 0.1)` with 10 random samples. Again, use 3-fold CV and the performance measure is MSE. What are the best hyperparameters and what is the MSE in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='sigmoid', max_iter=50000, random_state=42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_rbf_clf = SVC(kernel='rbf',max_iter=50000,random_state=42)\n",
    "svm_rbf_clf.fit(X, y)\n",
    "\n",
    "svm_sig_clf = SVC(kernel='sigmoid',max_iter=50000,random_state=42)\n",
    "svm_sig_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] C=8.285434042921391, gamma=0.023838534578000857 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. C=8.285434042921391, gamma=0.023838534578000857, total= 5.5min\n",
      "[CV] C=8.285434042921391, gamma=0.023838534578000857 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. C=8.285434042921391, gamma=0.023838534578000857, total=10.3min\n",
      "[CV] C=8.285434042921391, gamma=0.023838534578000857 .................\n",
      "[CV] .. C=8.285434042921391, gamma=0.023838534578000857, total= 5.9min\n",
      "[CV] C=10.695461949946674, gamma=0.010708043206672069 ................\n",
      "[CV] . C=10.695461949946674, gamma=0.010708043206672069, total= 6.3min\n",
      "[CV] C=10.695461949946674, gamma=0.010708043206672069 ................\n",
      "[CV] . C=10.695461949946674, gamma=0.010708043206672069, total= 5.7min\n",
      "[CV] C=10.695461949946674, gamma=0.010708043206672069 ................\n",
      "[CV] . C=10.695461949946674, gamma=0.010708043206672069, total= 6.3min\n",
      "[CV] C=10.207024889110668, gamma=0.0811314013719687 ..................\n",
      "[CV] ... C=10.207024889110668, gamma=0.0811314013719687, total= 5.7min\n",
      "[CV] C=10.207024889110668, gamma=0.0811314013719687 ..................\n",
      "[CV] ... C=10.207024889110668, gamma=0.0811314013719687, total= 5.8min\n",
      "[CV] C=10.207024889110668, gamma=0.0811314013719687 ..................\n",
      "[CV] ... C=10.207024889110668, gamma=0.0811314013719687, total= 6.4min\n",
      "[CV] C=4.847913896271592, gamma=0.008215761459316151 .................\n",
      "[CV] .. C=4.847913896271592, gamma=0.008215761459316151, total= 5.7min\n",
      "[CV] C=4.847913896271592, gamma=0.008215761459316151 .................\n",
      "[CV] .. C=4.847913896271592, gamma=0.008215761459316151, total= 5.7min\n",
      "[CV] C=4.847913896271592, gamma=0.008215761459316151 .................\n",
      "[CV] .. C=4.847913896271592, gamma=0.008215761459316151, total= 7.4min\n",
      "[CV] C=2.9615156301997616, gamma=0.0010844455091726145 ...............\n",
      "[CV]  C=2.9615156301997616, gamma=0.0010844455091726145, total= 5.7min\n",
      "[CV] C=2.9615156301997616, gamma=0.0010844455091726145 ...............\n",
      "[CV]  C=2.9615156301997616, gamma=0.0010844455091726145, total= 6.7min\n",
      "[CV] C=2.9615156301997616, gamma=0.0010844455091726145 ...............\n",
      "[CV]  C=2.9615156301997616, gamma=0.0010844455091726145, total= 7.1min\n",
      "[CV] C=6.338013772825093, gamma=0.00838166127234242 ..................\n",
      "[CV] ... C=6.338013772825093, gamma=0.00838166127234242, total= 5.7min\n",
      "[CV] C=6.338013772825093, gamma=0.00838166127234242 ..................\n",
      "[CV] ... C=6.338013772825093, gamma=0.00838166127234242, total= 6.0min\n",
      "[CV] C=6.338013772825093, gamma=0.00838166127234242 ..................\n",
      "[CV] ... C=6.338013772825093, gamma=0.00838166127234242, total= 5.7min\n",
      "[CV] C=5.920901863355592, gamma=0.009215784236546891 .................\n",
      "[CV] .. C=5.920901863355592, gamma=0.009215784236546891, total= 5.7min\n",
      "[CV] C=5.920901863355592, gamma=0.009215784236546891 .................\n",
      "[CV] .. C=5.920901863355592, gamma=0.009215784236546891, total= 5.7min\n",
      "[CV] C=5.920901863355592, gamma=0.009215784236546891 .................\n",
      "[CV] .. C=5.920901863355592, gamma=0.009215784236546891, total= 6.3min\n",
      "[CV] C=8.425823161851639, gamma=0.010799741148609078 .................\n",
      "[CV] .. C=8.425823161851639, gamma=0.010799741148609078, total=21.2min\n",
      "[CV] C=8.425823161851639, gamma=0.010799741148609078 .................\n",
      "[CV] .. C=8.425823161851639, gamma=0.010799741148609078, total=18.7min\n",
      "[CV] C=8.425823161851639, gamma=0.010799741148609078 .................\n",
      "[CV] .. C=8.425823161851639, gamma=0.010799741148609078, total= 5.9min\n",
      "[CV] C=10.08338029598364, gamma=0.02973839101746801 ..................\n",
      "[CV] ... C=10.08338029598364, gamma=0.02973839101746801, total=34.4min\n",
      "[CV] C=10.08338029598364, gamma=0.02973839101746801 ..................\n",
      "[CV] ... C=10.08338029598364, gamma=0.02973839101746801, total= 5.9min\n",
      "[CV] C=10.08338029598364, gamma=0.02973839101746801 ..................\n",
      "[CV] ... C=10.08338029598364, gamma=0.02973839101746801, total=22.7min\n",
      "[CV] C=5.013341443069226, gamma=0.006241546312774898 .................\n",
      "[CV] .. C=5.013341443069226, gamma=0.006241546312774898, total= 5.5min\n",
      "[CV] C=5.013341443069226, gamma=0.006241546312774898 .................\n",
      "[CV] .. C=5.013341443069226, gamma=0.006241546312774898, total= 9.3min\n",
      "[CV] C=5.013341443069226, gamma=0.006241546312774898 .................\n",
      "[CV] .. C=5.013341443069226, gamma=0.006241546312774898, total= 5.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 260.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=SVC(max_iter=50000, random_state=42),\n",
       "                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fc5d989b160>,\n",
       "                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fc5d98954c0>},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\n",
    "rnd_search_cv = RandomizedSearchCV(svm_rbf_clf, param_distributions, n_iter=10, verbose=2, cv=3)\n",
    "rnd_search_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=6.338013772825093, gamma=0.00838166127234242, max_iter=50000,\n",
       "    random_state=42)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_estimator_.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM regression with a Gaussian RBF kernel best hyperparameters: C=6.338013772825093, gamma=0.00838166127234242."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] C=2.448115280243586, gamma=0.017040537081944925 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. C=2.448115280243586, gamma=0.017040537081944925, total=   3.0s\n",
      "[CV] C=2.448115280243586, gamma=0.017040537081944925 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. C=2.448115280243586, gamma=0.017040537081944925, total=   2.9s\n",
      "[CV] C=2.448115280243586, gamma=0.017040537081944925 .................\n",
      "[CV] .. C=2.448115280243586, gamma=0.017040537081944925, total=   2.8s\n",
      "[CV] C=4.589679817939181, gamma=0.02676376674044744 ..................\n",
      "[CV] ... C=4.589679817939181, gamma=0.02676376674044744, total=   2.9s\n",
      "[CV] C=4.589679817939181, gamma=0.02676376674044744 ..................\n",
      "[CV] ... C=4.589679817939181, gamma=0.02676376674044744, total=   2.9s\n",
      "[CV] C=4.589679817939181, gamma=0.02676376674044744 ..................\n",
      "[CV] ... C=4.589679817939181, gamma=0.02676376674044744, total=   2.8s\n",
      "[CV] C=5.856192492245775, gamma=0.0018525202815017476 ................\n",
      "[CV] . C=5.856192492245775, gamma=0.0018525202815017476, total=   2.9s\n",
      "[CV] C=5.856192492245775, gamma=0.0018525202815017476 ................\n",
      "[CV] . C=5.856192492245775, gamma=0.0018525202815017476, total=   2.9s\n",
      "[CV] C=5.856192492245775, gamma=0.0018525202815017476 ................\n",
      "[CV] . C=5.856192492245775, gamma=0.0018525202815017476, total=   2.8s\n",
      "[CV] C=4.710333583978392, gamma=0.002052606529592214 .................\n",
      "[CV] .. C=4.710333583978392, gamma=0.002052606529592214, total=   2.8s\n",
      "[CV] C=4.710333583978392, gamma=0.002052606529592214 .................\n",
      "[CV] .. C=4.710333583978392, gamma=0.002052606529592214, total=   2.8s\n",
      "[CV] C=4.710333583978392, gamma=0.002052606529592214 .................\n",
      "[CV] .. C=4.710333583978392, gamma=0.002052606529592214, total=   2.8s\n",
      "[CV] C=8.932236522780656, gamma=0.04822904564993589 ..................\n",
      "[CV] ... C=8.932236522780656, gamma=0.04822904564993589, total=   2.8s\n",
      "[CV] C=8.932236522780656, gamma=0.04822904564993589 ..................\n",
      "[CV] ... C=8.932236522780656, gamma=0.04822904564993589, total=   2.8s\n",
      "[CV] C=8.932236522780656, gamma=0.04822904564993589 ..................\n",
      "[CV] ... C=8.932236522780656, gamma=0.04822904564993589, total=   2.8s\n",
      "[CV] C=2.7496078182688124, gamma=0.00665101941068922 .................\n",
      "[CV] .. C=2.7496078182688124, gamma=0.00665101941068922, total=   2.8s\n",
      "[CV] C=2.7496078182688124, gamma=0.00665101941068922 .................\n",
      "[CV] .. C=2.7496078182688124, gamma=0.00665101941068922, total=   2.9s\n",
      "[CV] C=2.7496078182688124, gamma=0.00665101941068922 .................\n",
      "[CV] .. C=2.7496078182688124, gamma=0.00665101941068922, total=   2.9s\n",
      "[CV] C=10.913200054186685, gamma=0.02369679689894758 .................\n",
      "[CV] .. C=10.913200054186685, gamma=0.02369679689894758, total=   2.8s\n",
      "[CV] C=10.913200054186685, gamma=0.02369679689894758 .................\n",
      "[CV] .. C=10.913200054186685, gamma=0.02369679689894758, total=   2.8s\n",
      "[CV] C=10.913200054186685, gamma=0.02369679689894758 .................\n",
      "[CV] .. C=10.913200054186685, gamma=0.02369679689894758, total=   2.8s\n",
      "[CV] C=2.805821551894735, gamma=0.0029180417507981143 ................\n",
      "[CV] . C=2.805821551894735, gamma=0.0029180417507981143, total=   2.8s\n",
      "[CV] C=2.805821551894735, gamma=0.0029180417507981143 ................\n",
      "[CV] . C=2.805821551894735, gamma=0.0029180417507981143, total=   2.8s\n",
      "[CV] C=2.805821551894735, gamma=0.0029180417507981143 ................\n",
      "[CV] . C=2.805821551894735, gamma=0.0029180417507981143, total=   2.8s\n",
      "[CV] C=4.662983663823204, gamma=0.004614784533890433 .................\n",
      "[CV] .. C=4.662983663823204, gamma=0.004614784533890433, total=   2.8s\n",
      "[CV] C=4.662983663823204, gamma=0.004614784533890433 .................\n",
      "[CV] .. C=4.662983663823204, gamma=0.004614784533890433, total=   2.8s\n",
      "[CV] C=4.662983663823204, gamma=0.004614784533890433 .................\n",
      "[CV] .. C=4.662983663823204, gamma=0.004614784533890433, total=   2.8s\n",
      "[CV] C=4.354858796928117, gamma=0.0031047507523971345 ................\n",
      "[CV] . C=4.354858796928117, gamma=0.0031047507523971345, total=   2.8s\n",
      "[CV] C=4.354858796928117, gamma=0.0031047507523971345 ................\n",
      "[CV] . C=4.354858796928117, gamma=0.0031047507523971345, total=   2.8s\n",
      "[CV] C=4.354858796928117, gamma=0.0031047507523971345 ................\n",
      "[CV] . C=4.354858796928117, gamma=0.0031047507523971345, total=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=SVC(kernel='sigmoid', max_iter=50000,\n",
       "                                 random_state=42),\n",
       "                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fc5d989b160>,\n",
       "                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fc5d98954c0>},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_search_cv = RandomizedSearchCV(svm_sig_clf, param_distributions, n_iter=10, verbose=2, cv=3)\n",
    "sig_search_cv.fit(X_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=2.448115280243586, gamma=0.017040537081944925, kernel='sigmoid',\n",
       "    max_iter=50000, random_state=42)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_search_cv.best_estimator_.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM regression with a sigmoid kernel best hyperparameters: C=2.448115280243586, gamma=0.017040537081944925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rnd = rnd_search_cv.best_estimator_.predict(X_test)\n",
    "mse_rnd = mean_squared_error(y_test,y_pred_rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34019753706.484013"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sig = sig_search_cv.best_estimator_.predict(X_test)\n",
    "mse_sig = mean_squared_error(y_test,y_pred_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34019753706.484013"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF kernel with best hyperparameters got MSE = 34019753706.484013 and sigmoid kernel got MSE = 34019753706.484013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) [2 marks]\n",
    "\n",
    "**Task:** Choose the best model in (d). Then for this model, evaluate the RMSE on the test set, which is stored in `X_test` and `y_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we choose sigmoid kernel since it has smaller MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = sig_search_cv.predict(X_test)\n",
    "RMSE = np.sqrt(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE of sigmoid kernel is 114752.8087714515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Voting Classifiers [8 marks]\n",
    "## (a)  [4 marks]\n",
    "\n",
    "Consider the MNIST dataset. To save computational time, split it into a smaller training set (the first 5000 observations) and a validation set (the next 1000 observations) as given by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "M = 6000\n",
    "X_train = mnist[\"data\"][:N]\n",
    "X_val  = mnist[\"data\"][N:M]\n",
    "y_train = mnist[\"target\"][:N]\n",
    "y_val = mnist[\"target\"][N:M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not shuffle the data and do not use a standard scaler. Train the following classifiers on the training set:\n",
    "\n",
    "(i) a random forest classifier with arguments `n_estimators=100, n_jobs=-1, random_state=42`,\n",
    "\n",
    "(ii) an extra-trees classifier with arguments `n_estimators=100, n_jobs=-1, random_state=42`,\n",
    "\n",
    "(iii) an AdaBoost classifier `n_estimators=50, learning_rate=0.2, random_state=42`,\n",
    "\n",
    "(iv) a gradient boosting classifier using the class `GradientBoostingClassifier()` with arguments `max_depth=2, n_estimators=10, learning_rate=0.25, random_state=42`.\n",
    "\n",
    "Report the accuracy of each trained classifier on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.25, max_depth=2, n_estimators=10,\n",
       "                           random_state=42)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train,y_train)\n",
    "\n",
    "#ii\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ext_clf = ExtraTreesClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "ext_clf.fit(X_train,y_train)\n",
    "#iii\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(n_estimators=50, learning_rate=0.2, random_state=42)\n",
    "ada_clf.fit(X_train,y_train)\n",
    "#iv\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(max_depth=2, n_estimators=10, learning_rate=0.25, random_state=42)\n",
    "gb_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.939"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred_rnd = rnd_clf.predict(X_val)\n",
    "accuracy_score(y_val, y_pred_rnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of random forest classifier on the validation set is 0.939."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.947"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ext = ext_clf.predict(X_val)\n",
    "accuracy_score(y_val, y_pred_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of extra-trees classifier on the validation set is 0.947."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.736"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ada = ada_clf.predict(X_val)\n",
    "accuracy_score(y_val, y_pred_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of AdaBoost classifier on the validation set is 0.736."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.834"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_gb = gb_clf.predict(X_val)\n",
    "accuracy_score(y_val, y_pred_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of gradient boosting classifier on the validation set is 0.834."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)  [4 marks]\n",
    "Train a hard-voting and a soft-voting ensemble classifier based on the models in (a). Evaluate each voting classifier on the validation set. Comment on whether the performance of the ensemble model is better or worse than the individual models in (a) and why that is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf',\n",
       "                              RandomForestClassifier(n_jobs=-1,\n",
       "                                                     random_state=42)),\n",
       "                             ('et',\n",
       "                              ExtraTreesClassifier(n_jobs=-1, random_state=42)),\n",
       "                             ('ab',\n",
       "                              AdaBoostClassifier(learning_rate=0.2,\n",
       "                                                 random_state=42)),\n",
       "                             ('gb',\n",
       "                              GradientBoostingClassifier(learning_rate=0.25,\n",
       "                                                         max_depth=2,\n",
       "                                                         n_estimators=10,\n",
       "                                                         random_state=42))])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "eclf_hard = VotingClassifier(estimators=[('rf',rnd_clf), ('et',ext_clf),('ab',ada_clf),('gb',gb_clf)]\n",
    "                             , voting='hard')\n",
    "eclf_hard.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.923"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_hard = eclf_hard.predict(X_val)\n",
    "accuracy_score(y_val, y_pred_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf',\n",
       "                              RandomForestClassifier(n_jobs=-1,\n",
       "                                                     random_state=42)),\n",
       "                             ('et',\n",
       "                              ExtraTreesClassifier(n_jobs=-1, random_state=42)),\n",
       "                             ('ab',\n",
       "                              AdaBoostClassifier(learning_rate=0.2,\n",
       "                                                 random_state=42)),\n",
       "                             ('gb',\n",
       "                              GradientBoostingClassifier(learning_rate=0.25,\n",
       "                                                         max_depth=2,\n",
       "                                                         n_estimators=10,\n",
       "                                                         random_state=42))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "eclf_soft = VotingClassifier(estimators=[('rf',rnd_clf), ('et',ext_clf),('ab',ada_clf),('gb',gb_clf)]\n",
    "                             , voting='soft')\n",
    "eclf_soft.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.926"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_soft = eclf_soft.predict(X_val)\n",
    "accuracy_score(y_val, y_pred_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hard voting classifier has 0.923 accuracy and soft voting classifier has 0.926 accuracy.\n",
    "The hard voting and soft voting classifier accuracy worse than RandomForestClassifier and ExtraTreesClassifier, but voting classifier perform better than AdaBoostClassifier and GradientBoostingClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stacking [9 marks]\n",
    "\n",
    "We continue with the setting of Question 3. The training set, validation set and test set are the same. In Question 3, we have used predetermined rules (that is, hard-voting and soft-voting) to build the ensemble prediction. **Stacking** is an ensemble method in which you train a model (called a **blender**) to aggregate the result of each predictor into an ensemble prediction.\n",
    "\n",
    "Hint: Reading the subsection \"Stacking\" in Chapter 7 of the textbook and the solution to Question 9 in the Chapter 7 [Jupyter notebook](https://github.com/ageron/handson-ml2/blob/master/07_ensemble_learning_and_random_forests.ipynb) on the textbook website may help with this question.\n",
    "\n",
    "## (a)  [3 marks]\n",
    "\n",
    "For each of the four classifiers in Question 3(a), make 5000 clean predictions on the training set with 3-fold cross validation using `sklearn.model_selection.cross_val_predict()`. You should end up with four predictions per observation. Print at least the first 5 rows of `pred`. Next, apply one-hot encoding to `pred` since these predictions are class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred_rf = cross_val_predict(rnd_clf,X_train, y_train, cv=3)\n",
    "\n",
    "y_train_pred_et = cross_val_predict(ext_clf,X_train, y_train, cv=3)\n",
    "\n",
    "y_train_pred_ab = cross_val_predict(ada_clf,X_train, y_train, cv=3)\n",
    "\n",
    "y_train_pred_gb = cross_val_predict(gb_clf,X_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 5, 3, 3], [0, 0, 5, 0], [4, 4, 4, 4], [1, 1, 1, 1], [9, 9, 9, 9]]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = []\n",
    "for k in range(5000):   \n",
    "    pred.append([])\n",
    "    pred[k].append(y_train_pred_rf[k])\n",
    "    pred[k].append(y_train_pred_et[k])\n",
    "    pred[k].append(y_train_pred_ab[k])\n",
    "    pred[k].append(y_train_pred_gb[k])\n",
    "pred[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot = OneHotEncoder()\n",
    "pred = onehot.fit_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1.0\n",
      "  (0, 15)\t1.0\n",
      "  (0, 23)\t1.0\n",
      "  (0, 33)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 10)\t1.0\n",
      "  (1, 25)\t1.0\n",
      "  (1, 30)\t1.0\n",
      "  (2, 4)\t1.0\n",
      "  (2, 14)\t1.0\n",
      "  (2, 24)\t1.0\n",
      "  (2, 34)\t1.0\n",
      "  (3, 1)\t1.0\n",
      "  (3, 11)\t1.0\n",
      "  (3, 21)\t1.0\n",
      "  (3, 31)\t1.0\n",
      "  (4, 9)\t1.0\n",
      "  (4, 19)\t1.0\n",
      "  (4, 29)\t1.0\n",
      "  (4, 39)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [3 marks]\n",
    "Use the predictions in (a) as features and the actual label of the observations as the target. Train a random forest classifier on the training set with the parameters `n_estimators=100, random_state=42`.  This classifier is a blender. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9268"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "blender = rf.fit(pred,y_train)\n",
    "blender.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [3 marks]\n",
    "\n",
    "Obtain the predictions of the blender on the validation set by feeding predictions on the validation set from the four classifiers in Question 3(a) into the blender trained in Question 4(b). Do not retrain the blender. These are called stacking predictions. Report the accuracy of your stacking predictions on the validation set and compare this to the results in Question 3(b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf.fit(X_train,y_train)\n",
    "rnd_pred = rnd_clf.predict(X_val)\n",
    "\n",
    "ext_clf.fit(X_train,y_train)\n",
    "et_pred = ext_clf.predict(X_val)\n",
    "\n",
    "ada_clf.fit(X_train,y_train)\n",
    "ab_pred = ada_clf.predict(X_val)\n",
    "\n",
    "gb_clf.fit(X_train,y_train)\n",
    "gb_pred = gb_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_c = []\n",
    "for k in range(1000):   \n",
    "    pred_c.append([])\n",
    "    pred_c[k].append(rnd_pred[k])\n",
    "    pred_c[k].append(et_pred[k])\n",
    "    pred_c[k].append(ab_pred[k])\n",
    "    pred_c[k].append(gb_pred[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 7, 7, 7], [3, 3, 3, 3], [4, 4, 9, 9], [6, 6, 6, 6], [1, 1, 1, 1]]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_c[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947\n"
     ]
    }
   ],
   "source": [
    "my_pred = onehot.fit_transform(pred_c)\n",
    "pred = blender.predict(my_pred)\n",
    "accuracy = accuracy_score(y_val,pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of your stacking predictions on the validation set is 0.947. It is litte bit better than the result in question 3b."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
